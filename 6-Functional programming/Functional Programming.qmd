---
title: "Functional programming"
author: "Rakesh Poduval"
description: "Workshop series in R programming"
date: 9/16/2023
format:
  html:
    theme: default
    page-layout: article
    grid:
      sidebar-width: 250px
      body-width: 800px
      margin-width: 250px
      gutter-width: 1.5em
    smooth-scroll: true
    anchor-sections: true
    toc: true
    toc-expand: 2
    toc-depth: 3
    toc-location: left
    number-sections: true
    number-depth: 3
    code-fold: false
    code-summary: "show code"
    code-annotations: below
    code-copy: true
    code-tools:
      source: true
      toggle: false
      caption: "show code"
    code-line-numbers: true
    df-print: kable
    fig-cap-location: bottom
editor: visual
editor_options: 
  chunk_output_type: console
---

::: column-margin

![](logo_CUSAT.png){#fig-logo width="80" height="80"}

:   DOS-OSF
:::

> R, at its heart, is a functional language which lends itself to a problem solving centred on **functions**. A *functional style* is about decomposing a big problem into smaller pieces, then solving each piece with a function. The three keys functional techniques for this are: *`functionals`*, *`function factories`* & *`function operators`*.

# Functionals

## Introduction

A **functional** is a function that takes a function as an input and returns a vector as output. A common use of **functional** is as an alternative to for loops since loops have a bad rap for being slow, although they're very flexible. Check out an simple examples below.

Calculating mean of random numbers generated:

``` r
randomise <- function(f, n = 1000) f(runif(n, min = 0, max = 1))
randomise(mean) # iterate
```

## Motivation

### Scenario 1

::: column-margin
> Replace all missing values (`NA`s) with 0
:::

```{r, include=TRUE}
#| label: tbl-dd
#| tbl-cap: Sample data with missings
#| tbl-colwidths: [30,70]
#| column: margin
set.seed(1010) # generates same sample
dd <- setNames(data.frame(replicate(6, sample(c(1:10, NA), 6, rep = TRUE))), letters[1:6])
dd
```

::: panel-tabset
#### copy-paste

``` r
dd$b[is.na(dd$b)] <- 0
dd$c[is.na(dd$c)] <- 0
dd$f[is.na(dd$f)] <- 0
```

#### DRY-principle

``` r
replaceNA <- function(x, replace_with = 0) {
  x[is.na(x)] <- replace_with
  x
}
dd$b <- replaceNA(dd$b)
dd$c <- replaceNA(dd$c)
dd$f <- replaceNA(dd$f)
```

#### loop

``` r
replaceNA <- function(x, replace_with = 0) {
  x[is.na(x)] <- replace_with
  x
}
for(i in 1:ncol(dd)) {
  dd[, i] <- replaceNA(dd[, i])
}
```

#### functionals-lapply

``` r
replaceNA <- function(x, replace_with = 0) {
  x[is.na(x)] <- replace_with
  x
}
dd[] <- lapply(dd, replaceNA)
# dd[c(2, 3, 6)] <- lapply(dd[c(2, 3, 6)], replaceNA)
```
:::

This code `functionals-lapply` has five advantages over others:

-   It's more compact.
-   If the code for a missing value changes, it only needs to be updated in one place.
-   It works for any number of columns. There is no way to accidentally miss a column.
-   There is no way to accidentally treat one column differently than another.
-   It is easy to generalise this technique to a subset of columns:

### Scenario 2

Refer @tbl-dd

::: column-margin
> compute the same set of descriptive statistics for each variable
:::

::: panel-tabset
#### copy-paste

``` r
min(dd$a, na.rm = TRUE) # minimum
mean(dd$a, na.rm = TRUE) # average 
median(dd$a, na.rm = TRUE) # median 
max(dd$a, na.rm = TRUE) # maximum
sd(dd$a, na.rm = TRUE) # standard deviation

# ... repeat the same for rest of the columns of dd
```

#### DRY-principle

``` r
descriptive_stats <- function(x, ...) {
  c(min = min(x, ...), avg = mean(x, ...), 
    med = median(x, ...), 
    max = max(x, ...), std = sd(x, ...))
  }
descriptive_stats(dd$a, na.rm = TRUE)
descriptive_stats(dd$b, na.rm = TRUE)
descriptive_stats(dd$c, na.rm = TRUE)

# ... repeat the same for rest of the columns of dd
```

#### functionals-lapply

``` r
descriptive_stats <- function(x, ...) {
  c(min = min(x, ...), avg = mean(x, ...),
    med = median(x, ...), 
    max = max(x, ...), std = sd(x, ...))
  }
  
data.frame(lapply(dd, descriptive_stats, na.rm = TRUE))
data.frame(apply(dd, 2, descriptive_stats)) # alternatively
```
:::

### Scenario 3

::: column-margin
> comparing performace between `for` loop & `apply` method with `t.test`.
:::

``` {r}
#| label: fig-perf
#| column: margin
#| code-fold: false
#| fig-cap: "loops vs. apply" 
dd <- as.data.frame(matrix(rnorm(1000 * 50, mean = 10, sd = 3), nrow = 1000))
dd$group <- sample(1:2, size = 1000, replace = TRUE)
# head(dd)
# prop.table(table(dd$group))

vars <- setdiff(names(dd), "group")

perf <- microbenchmark::microbenchmark(
  loop = for(i in vars) t.test(reformulate("group", i), dd),
  apply = apply(dd[vars], 2, function(x) t.test(x, dd["group"])),
  times = 100)

library(ggplot2)
autoplot(perf) + theme_grey(base_size = 25)
```

## Fundamental concept

We will focus on *`functionals`* provided by the [purrr](https://purrr.tidyverse.org) package. These functions (`purrr:::map()`) have a consistent interface that makes it easier to understand the key ideas than their base equivalents.

::: column-margin

![](logo_purrr.png){width="80" height="80"}

:   
:::

::: {#fig-concept layout="[[42,58], [46, 54]]"}
![calls the function once for each element](diagrams/map.png){#fig-map}

![(a) with single argument of the f()](diagrams/map-arg.png){#fig-maparg}

![(a) with multi argument of the f()](diagrams/map-arg-recycle.png){#fig-mapargrecycle}

![calls the function once for each list](diagrams/map-list.png){#fig-maplist}

**Design of `purrr:::map()`**
:::

::: column-margin
> All map functions always return an output vector the same length as the input. Here **`map()`** refers to *"an operation that associates each element of a given set with one or more elements of a second set"*. This makes sense here because `map()` defines a mapping from one vector to another.
:::

## Base vs. purrr

``` r
library(purrr)
```

The base equivalent to `map()` is `lapply()`. The only difference is that `map()` has four more specific variants: `map_lgl()`, `map_int()`, `map_dbl()`, and `map_chr()` which returns an atomic vector of the specified type and not a list like `lapply()`. Base R has two apply functions `sapply()` & `vapply()`. Avoid `sapply()` because it tries to simplify the result, which makes it difficult to program with. If you don't want to use `purrr`, it is recommend to use `vapply()`. For example, the equivalent to `map_dbl(x, mean, na.rm = TRUE)` is `vapply(x, mean, na.rm = TRUE, FUN.VALUE = double(1))`.

::: panel-tabset
### purrr

``` r
# vector operation
map(1:3, function(x) x * 2) 
map(1:3, ~ runif(2)) # all purrr functions translate formulas into function

# data.frame
map(mtcars, mean) 
map_dbl(mtcars, mean)
map_dbl(mtcars, function(x) length(unique(x)))
map_dbl(mtcars, ~ length(unique(.x)))
```

### base

``` r
# vector operation
lapply(1:3, unction(x) x * 2)
lapply(1:3, function(x) runif(2))

# data.frame
lapply(mtcars, mean) 
apply(mtcars, 2, mean)
apply(mtcars, 2, function(x) length(unique(x)))
```
:::

::: callout-warning
One needs to know the expected output type. It's often useful to switch back to `map()`, because it can accept any type of output.
:::

## Extractions

The map functions have shortcuts for extracting elements from a vector, powered by `purrr::pluck()`. You can use

-   a character vector to select elements by name
-   an integer vector to select by position or
-   a list to select by both name and position.

``` r
x <- list(
  list(-1, x = 1, y = c(2), z = "a"),
  list(-2, x = 4, y = c(5, 6), z = "b"),
  list(-3, x = 8, y = c(9, 10, 11)))
x
```

::: panel-tabset
### purrr

``` r
map_dbl(x, "x") # select by name
map_dbl(x, 1) # select by position
map_dbl(x, list("y", 1)) # select by both
map_chr(x, "z") # error if a component doesn't exist
map_chr(x, "z", .default = NA)
unlist(map(x, "z")) # the safer way
```

### base

``` r
as.numeric(unlist(lapply(x, function(e) e["x"]))) # select by name
as.numeric(unlist(lapply(x, function(e) e[1]))) # select by position
as.numeric(lapply(x, function(e) e["y"][[1]][1])) # select by both
as.character(unlist(lapply(x, function(e) e["z"])))
```
:::

## Passing arguments

It's often required to pass additional arguments to the function that is called.

``` r
(x <- list(1:5, c(1:10, NA))
map_dbl(x, ~ mean(.x)) # NA
map_dbl(x, ~ mean(.x, na.rm = TRUE))
map_dbl(x, mean, na.rm = TRUE) # simpler form
```

::: column-margin
> Any arguments that come after `f` in the call are inserted ***after*** the data in individual calls to `f()`. Check @fig-maparg. Note that these arguments are only vectorised over its first argument. If an argument after `f` is a vector, it will be passed along as is @fig-maplist.
:::

::: callout-important
It is recommended writing out the full argument names, as it makes it easier to read.
:::

So far the first argument to `map()` has always been the first argument. Imagine a scenario where the first argument should be constant, and a second argument is varying. Refer @fig-mapargflipped. For example let's say we have a vector that contains a few unusual values, and we want to explore the effect of different amounts of trimming when computing the mean.

::: column-margin
![Varying a second argument](diagrams/map-arg-flipped.png){#fig-mapargflipped}
:::

``` r
trims <- c(0, 0.1, 0.2, 0.5)
x <- rcauchy(1000) # Cauchy Distribution
map_dbl(trims, ~ mean(x, trim = .x))
map_dbl(trims, function(t) mean(x, trim = t))
```

### Excercise

Let's use `purrr` functions to solve a moderately realistic problem. We will fit a model using `iris` to each subgroup `Species` and extract the `R2` & `SE` from each model.

::: panel-tabset
#### purrr

```{r, message=FALSE}
#| label: tbl-modelperf1
#| tbl-cap: model performance
#| code-fold: false
#| column: margin
library(dplyr)
library(purrr)

split(iris[, 1:4], iris$Species) %>% # <1>
  map(~ lm(Sepal.Length ~ ., data = .x)) %>% # <2>
  map(summary) %>% # <3>
  map(function(x) c(intercept = coef(x)[["(Intercept)", "Estimate"]], # <3>
                    se = x[["sigma"]], R2 = x[["r.squared"]], # <3>
                    R2adj = x[["adj.r.squared"]])) %>% # <3>
  map(round, 2) %>% # <4>
  bind_rows(.id = "model") # <5>
```

1.  Splitting `iris` by `Species` - creates a list of 4 `data.frames`
2.  Fit a linear model for each data split
3.  Extract performance metrics
4.  Rounding the results up to 2 decimals.
5.  Bind them to a single `data.frame`

#### base

```{r, message=FALSE}
#| label: tbl-modelperf2
#| tbl-cap: model performance
#| code-fold: false
#| column: margin
library(dplyr)

split(iris[, 1:4], iris$Species) %>%
  lapply(function(x) lm(Sepal.Length ~ ., data = x)) %>% 
  lapply(summary) %>% 
  lapply(function(x) c(intercept = coef(x)[["(Intercept)", "Estimate"]], 
                       se = x[["sigma"]], R2 = x[["r.squared"]], 
                       R2adj = x[["adj.r.squared"]])) %>% 
  lapply(round, 2) %>%
  bind_rows(.id = "model") 
```
:::

## Map family

There are 23 primary variants of `map()` but fortunately, by design of `purrr` we only need to learn five new ideas:

-   `modify()`: Returns same type as input
-   `map2()`: Iterate over two inputs
-   `imap()`: Iterate with an index.
-   `walk()`: Return nothing
-   `pmap()`: Iterate over any number of inputs

::: column-margin
| arguments | list     | same_type   | nothing   |
|-----------|----------|-------------|-----------|
| 1         | `map()`  | `modify()`  | `walk()`  |
| 2         | `map2()` | `modify2()` | `walk2()` |
| 1 + index | `imap()` | `imodify()` | `iwalk()` |
| N         | `pmap()` | ---         | `pwalk()` |

: Relation between variants {#tbl-mapvariants}
:::

`map()` always returns a list. `modify()` returns the same type of output as the input:

``` r
dd <- data.frame(x = 1:3, y = 6:4)

map(dd, ~ .x * 2) # list
modify(dd, ~ .x * 2) # data.frame
```

`modify()` has a very useful variant of called `modify_if()` which allows you to (e.g.) only double *numeric* columns of a data frame with `modify_if(dd, is.numeric, ~ .x * 2)`.

``` r
dd <- data.frame(x = c(0, 10, 20), y = c(5, 6, 7), 
                 z = c("a", "b", "c"), stringsAsFactors = FALSE)
map_if(dd, is.numeric, mean)
modify_if(dd, is.numeric, mean)
```

::: callout-note
`modify()` doesn't modify in place, it returns a modified copy, to make it permanent modify one needs to assign it.
:::

`map()` is vectorised over a single argument, `.x` thus making it poorly suited for some problems. For example, to find a weighted mean when you have a list of observations and a list of weights we need a new tool: a `map2()`, which is vectorised over two arguments.

::: column-margin
![map2](diagrams/map2.png){#fig-map2}
:::

``` r
x <- map(1:8, ~ runif(10)) # uniform random numbers
w <- map(1:8, ~ rpois(10, 5) + 1) # poisson random numbers
map_dbl(x, weighted.mean, w = w) # error
map2_dbl(x, w, weighted.mean) # desired output
```

The implementation of `map2()` is quite similar to `map()`, here instead of iterating over one vector, it iterate over two in parallel. One of the big differences between `map2()` & `map2()` is that it recycles its inputs to make sure that they're the same length.

::: callout-tip
`purrr` provides the walk family of functions that ignore the return values of the `.f` and instead return `.x` invisibly
:::

`imap()` family is analogous to the to the loop over form `for (i in seq_along(x))` & `for (nm in names(x))`. This allows you to iterate over the values and the indices of a vector in parallel.

``` r
x <- map(1:6, ~ sample(1000, 10))
imap_chr(x, ~ paste0("The highest value of ", .y, " is ", max(.x)))
```

::: callout-tip
`imap()` is useful if you want to work with the values in a vector along with their positions.
:::

Since we have `map()` and `map2()`, one might expect `map3()`, `map4()`, `map5()`, instead `purrr` takes a slightly different tack with `pmap()`: you supply it a single list, which contains any number of arguments. In most cases, that will be a list of equal-length vectors, similar to `data.frame`.

::: column-margin
![pmap](diagrams/pmap.png){#fig-pmap}
:::

::: {.callout-note title="Example"}
`map2(x, y, f)` is equivalent to `pmap(list(x, y), f)` `map2_dbl(x, w, weighted.mean)` is equivalent to `pmap_dbl(list(x, w), weighted.mean)`
:::

A big difference between `pmap()` and the is that `pmap()` gives you much finer control over argument matching over other map functions because one can name the components of the list.I It's always a good practice to name the components of the list to make it very clear how the function will be called.

::: column-margin
![pmap with arguments](diagrams/pmap-3.png){#fig-pmapargs}
:::

``` r
params <- tibble::tribble(
  ~ n, ~ min, ~ max,
   1L,     0,     1,
   2L,    10,   100,
   3L,   100,  1000)
   
pmap(params, runif)
```

::: {.callout-warning title="Caution"}
Here, the column names are critical.
:::

## Reduce family

The next most important family of functions is the `reduce` family. This is smaller, with only two main variants & is less commonly used, but it's a powerful idea. `reduce()` takes a vector of length *n* and produces a vector of length *1* by calling a function with a pair of values at a time.

::: column-margin
![reduce](diagrams/reduce.png){#fig-reduce}
:::

::: {.callout-note title="Example"}
`reduce(1:4, f)` is equivalent to `f(f(f(1, 2), 3), 4)`.
:::

```{r}
x <- map(1:4, ~ sample(1:10, 10, replace = T))
str(x)
```

Let's find the common observations from the above list

::: column-margin
The first `reduce()` variant is `accumulate()` which it returns all the intermediate results.
:::

::: panel-tabset
### reduce

```{r}
#| code-fold: false
# accumulate(x, intersect)
reduce(x, intersect)
```

### copy-paste

```{r}
#| code-fold: false
out <- x[[1]]
out <- intersect(out, x[[2]])
out <- intersect(out, x[[3]])
out <- intersect(out, x[[4]])
out
```

### loop

```{r}
#| code-fold: false
out <- x[[1]]
for (i in 2:length(x)) {
  out <- intersect(out, x[[i]])
}
out
```
:::

::: column-margin
> `map-reduce` (map combined with a reduce) is the idea that powers technology like `Hadoop`. Now you can see how simple and powerful the underlying idea is. As a simple example, imagine computing the mean of a very large vector, so large that it has to be split over multiple computers. You could ask each computer to calculate the sum and the length, and then return those to the coordinator which computes the overall mean by dividing the total sum by the total length.
:::

## Predicate functionals

A **predicate** function returns a single `TRUE` or `FALSE`, it returns `TRUE` when it **matches** a vector. It applies a predicate to each element of a vector. `purrr` provides seven useful functions which come in three groups:

|     | function               | description                                 |
|-----|------------------------|---------------------------------------------|
| 1   | `some(.x, .p)`         | returns `TRUE` if **any** element matches   |
|     | `every(.x, .p)`        | returns `TRUE` if **all** element matches.  |
|     | `none(.x, .p)`         | returns `TRUE` if **no** element matches    |
| 2   | `detect(.x, .p)`       | returns the **value** of the first match    |
|     | `detect_index(.x, .p)` | returns the **location** of the first match |
| 3   | `keep(.x, .p)`.        | **keeps** all matching elements             |
|     | `discard(.x, .p)`      | **drops** all matching elements             |

``` r
dd <- data.frame(x = 1:3, y = c("a", "b", "c"), stringsAsFactors = TRUE)
detect(dd, is.factor)
detect_index(dd, is.factor)
keep(dd, is.factor)
discard(dd, is.factor)
```

::: callout-note
`map()` and `modify()` also can take predicate functions.
:::

# Applications

## MLE

The goal of maximum likelihood estimation (MLE) is to find the parameter values for a distribution that make the observed data most likely. To do MLE, we start with a probability function. For example, let's take the Poisson distribution. If we know $\lambda$, we can compute the probability of getting a vector $\mathbf{x}$ of values ($x_1$, $x_2$, ..., $x_n$) by multiplying the Poisson probability function as follows:

$$
P(\lambda, \mathbf{x}) = \prod_{i=1}^{n} \frac{\lambda ^ {x_i} e^{-\lambda}}{x_i!}
$$ Let's apply a `log` transformation to this probability function and simplify it as much as possible: $$
\log(P(\lambda, \mathbf{x})) = \log(\lambda) \sum_{i=1}^{n} x_i - n \lambda - \sum_{i=1}^{n} \log(x_i!) 
$$

We can now turn this function into an R function.

``` r
#' log_likelihood
#' @param lambda shape parameter 
#' @param x input vector
#' @return compute the (logged) probability
#' @examples
#' log_likelihood(10, x = c(41, 30, 31, 38, 29))
#' log_likelihood(20, x = c(41, 30, 31, 38, 29))
#' log_likelihood(30, x = c(41, 30, 31, 38, 29))
#' 
log_likelihood <- function(lambda, x) {
  n <- length(x)
  (log(lambda) * sum(x)) - (n * lambda) - sum(lfactorial(x))
}
```

::: column-margin
> The R function is quite elegant because R is vectorised and, because it's a statistical programming language, R comes with built-in functions like the log-factorial (`lfactorial()`).
:::

So far we've been thinking of `lambda` as fixed & known & the function gives us the *probability of getting different values of `x`*. But in real-life, we observe `x` and it is `lambda` that is unknown. We want to find the `lambda` that makes the observed `x` the most likely. That is, given `x`, what value of `lambda` gives us the highest value of `log_likelihood`()?

``` r
X <- rpois(n = 1000, lambda = 32) # <1>
lambda <- runif(50, 0, 100) # <1>

M <- map_dbl(lambda, ~log_likelihood(lambda = .x, X)) # <2>
best_value <- lambda[M == reduce(M, max)] # <2>

# alternate approach
optimise(log_likelihood, interval = c(0, 100), x = X, maximum = TRUE) # <3>
```
1. Generate poisson random numbers & selecting a random set of `lambda`
3. Finding the best value.
4. Automates the process of finding the best value. It will evaluate `log_likelihood()` many times, using mathematical tricks to narrow in on the largest value as quickly as possible

# References

<https://purrr.tidyverse.org/reference/index.html>

